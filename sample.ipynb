{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeLatencyIndex():\n",
    "    \"\"\"Predictor class that predicts the efficiency of architecture given the accuracy predictor, \n",
    "    arthemetic intensity precictor and latency predictor. A new parameter is calculated and we call is compute_latency_index(cli)\n",
    "    \n",
    "    ref: [paper]\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, accuracy_predictor, ai_predictor, latency_predictor, weights = [0.5, 0.5]):\n",
    "        self.ai = ai_predictor\n",
    "        self.lat = latency_predictor\n",
    "        self.acc = accuracy_predictor    #expects list of samples\n",
    "        self.wts = weights\n",
    "        \n",
    "    \n",
    "    def predict_efficiency(self, sample):   #Computes CLI of latency.\n",
    "        arth_int = 1/self.ai.predict_efficiency(sample)  #actualy returns 1/arth_intensity\n",
    "        latency = self.lat.predict_efficiency(sample)\n",
    "        acc = self.acc.predict_accuracy([sample]).item()\n",
    "        \n",
    "        cli =  self.wts[1]*arth_int/latency + self.wts[0]*acc\n",
    "        \n",
    "        return cli  #inverse in order to make it the minimization problem\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'ks': [7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7], 'e': [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], 'd': [4, 4, 4, 4, 4]}\n",
    "\n",
    "\n",
    "cfg['r']= [224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy predictor is ready!\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# accuracy predictor\n",
    "\n",
    "cuda_available = False\n",
    "\n",
    "from ofa.accuracy_predictor import AccuracyPredictor\n",
    "from ofa.flops_table import ArthIntTable\n",
    "\n",
    "\n",
    "accuracy_predictor = AccuracyPredictor(\n",
    "    pretrained=True,\n",
    "    device='cuda:0' if cuda_available else 'cpu'\n",
    ")\n",
    "\n",
    "print('The accuracy predictor is ready!')\n",
    "print(accuracy_predictor.model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the arthemetic_intensity lookup table (resolution=224)...\n",
      "Built the arthemetic_intensity lookup table (resolution=224)!\n",
      "The  Efficient Arthemetic intensity predictor is ready!\n"
     ]
    }
   ],
   "source": [
    "from fpga_utils.latency_estimation import LatencyTable  \n",
    "\n",
    "arthemetic_intensity_lookup = ArthIntTable(pred_type='arthemetic_intensity', \n",
    "                                  device='cuda:0' if cuda_available else 'cpu',batch_size=1, \n",
    "                                  )\n",
    "\n",
    "latency_estimator = LatencyTable()\n",
    "\n",
    "print('The  Efficient Arthemetic intensity predictor is ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = ComputeLatencyIndex(accuracy_predictor, arthemetic_intensity_lookup, latency_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7384103622795521"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_efficiency(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supernetwork Ready\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from ofa.imagenet_classification.elastic_nn.networks.ofa_mbv3 import OFAMobileNetV3\n",
    "from ofa.model_zoo import ofa_net\n",
    "from ofa.utils import download_url\n",
    "\n",
    "net_id  = 'ofa_mbv3_d234_e346_k357_w1.2'\n",
    "url_base = \"https://raw.githubusercontent.com/han-cai/files/master/ofa/ofa_nets/\"\n",
    "\n",
    "ofa_network = OFAMobileNetV3(\n",
    "            dropout_rate=0,\n",
    "            width_mult=1.2,\n",
    "            ks_list=[3, 5, 7],\n",
    "            expand_ratio_list=[3, 4, 6],\n",
    "            depth_list=[2, 3, 4],\n",
    "        )\n",
    "\n",
    "pt_path = download_url(url_base + net_id, model_dir=\".torch/ofa_nets\")\n",
    "init = torch.load(pt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "ofa_network.load_state_dict(init)\n",
    "print('Supernetwork Ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ofa.evolution_finder import ArchManager\n",
    "eff = []\n",
    "samples = []\n",
    "arch =  ArchManager()\n",
    "for i in range(10000):\n",
    "    sample = arch.random_sample()\n",
    "    samples+=[sample]\n",
    "    eff+=[predictor.predict_efficiency(sample)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5248, 1.2582569625058737)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eff.index(min(eff)), max(eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wid': None,\n",
       " 'ks': [7, 7, 7, 5, 5, 7, 3, 3, 7, 7, 5, 7, 3, 7, 3, 5, 7, 3, 5, 3],\n",
       " 'e': [3, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 4, 3, 4, 3, 4, 6, 3, 6, 6],\n",
       " 'd': [4, 2, 2, 3, 4],\n",
       " 'r': [224]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[eff.index(max(eff))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'wid': None,\n",
       "  'ks': [5, 7, 7, 7, 3, 7, 3, 7, 7, 3, 7, 7, 3, 5, 7, 5, 3, 5, 5, 7],\n",
       "  'e': [3, 4, 3, 4, 3, 4, 4, 4, 6, 4, 3, 3, 3, 3, 4, 4, 3, 6, 6, 6],\n",
       "  'd': [2, 2, 2, 3, 4],\n",
       "  'r': [224]}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
